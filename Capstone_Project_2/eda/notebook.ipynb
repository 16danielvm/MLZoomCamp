{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People with or without masks detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1000)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAqACoDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDgPFkoufE7RIQyI52n6cV22i2SmyCmMYI5rz9ozfeIZhghgfu4HHfiumjfU7WFGtfNRQwUl5wfx2kVyVI82i3PTw8lDWWxrzeHoSWAjG0nOKgfQY7eJwkecjrmtPSJNX1KxmkOnzAxDhhGcSe4rAuZ9YuLjZNDKi52tCz+UdvqeKxUJ3O11aLjoc0ZTpurlCE2N2Y8ZroQ8RAO6IZ7A9K5zXLQ/NKYvKbOAN5Yfgahj1lRGoZVJwM10x1R5dSXLJml4RgF94witW+VZWJyD0HWvoSx062gs44zGHVBxlBXzjo7TaBrdnqjYaJXAdh0IPB/Q19KWDrJbRyxt8jICCPSuiNNJ3OKdWbVuhZiRo1JP3ccKO1cxrEUl4WN3pghhB2m5Lqzqex2gH5fqR9K6V3DDCNjFV5XIjZWwRjBB9PSqsiYylF3R4v450W9soGknSN4gOJIujen+f1ry3APPljn2r274k6yU0t7CLaZZhsCgfdX1rydLIbF4bp/drONPlNp15T3NfT7Uv4WvzIS0Z2GNTyVJOMAV674G8SLqPh+CBnH2m3QRSJnnI6GvNtNGPBFyw6+dEM+26q+myPDfO0TtGyykAqcYGa0k7WZlFXbR7pcai1k0KIgYu+HLLkAVQ1rWrbTrSS6uHEcYJPXGfYfWq1pI76fEzOzMRySc5ryLx1PNLq8kcksjovRWYkD8Kxi3zG0oLlRLM9x4hafVZtwM8+2FW/hjHb8xmrKaVd7FxCuMela0CqPDVgQo/1bdvcVIs0oRQJH6f3jW5xvc//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=42x42>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/data/Train/WithMask/'\n",
    "name = '4.png'\n",
    "fullname = f'{path}/{name}'\n",
    "fullname\n",
    "load_img(fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(fullname, target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(img)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Xception(weights= 'imagenet', input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 299, 299, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n03424325', 'gasmask', 0.054372024),\n",
       "  ('n04370456', 'sweatshirt', 0.0481796),\n",
       "  ('n03814639', 'neck_brace', 0.03075833),\n",
       "  ('n02667093', 'abaya', 0.026551317),\n",
       "  ('n03045698', 'cloak', 0.024661474)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing_function: funcion que se va a aplicar a cada imagen\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# batch_size: cuantas imagenes se van a procesar a la vez\n",
    "train_ds = train_gen.flow_from_directory('../data/data/Train', target_size= (150,150), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WithMask': 0, 'WithoutMask': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory('../data/data/Validation', target_size= (150,150), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "\n",
    "base_model.trainable = False # no se van a actualizar los pesos de la red\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(150,150,3)) # 150x150, 3 canales\n",
    "\n",
    "base = base_model(inputs, training=False) # no se va a entrenar\n",
    "\n",
    "vectors= keras.layers.GlobalAveragePooling2D()(base) # promedio de los vectores\n",
    "\n",
    "outputs = keras.layers.Dense(2)(vectors) # 2 clases \n",
    "\n",
    "model = keras.Model(inputs, outputs) # modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1 \n",
    "optimazer = keras.optimizers.Adam(learning_rate=learning_rate) # optimizador\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True) # funcion de perdida\n",
    "\n",
    "model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy']) # configuracion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "313/313 [==============================] - 300s 946ms/step - loss: 0.2059 - accuracy: 0.9901 - val_loss: 0.2855 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, validation_data=val_ds) # entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# preprocessing_function: funcion que se va a aplicar a cada imagen\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# batch_size: cuantas imagenes se van a procesar a la vez\n",
    "train_ds = train_gen.flow_from_directory('../data/data/Train', target_size= (224,224), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory('../data/data/Validation', target_size= (224,224), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "base_model.trainable = False # no se van a actualizar los pesos de la red\n",
    "\n",
    "inputs = keras.Input(shape=(224,224,3)) # 150x150, 3 canales\n",
    "\n",
    "base = base_model(inputs, training=False) # no se va a entrenar\n",
    "\n",
    "vectors= keras.layers.GlobalAveragePooling2D()(base) # promedio de los vectores\n",
    "\n",
    "outputs = keras.layers.Dense(2)(vectors) # 2 clases \n",
    "\n",
    "model = keras.Model(inputs, outputs) # modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1 \n",
    "optimazer = keras.optimizers.Adam(learning_rate=learning_rate) # optimizador\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=True) # funcion de perdida\n",
    "\n",
    "model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy']) # configuracion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 144s 454ms/step - loss: 0.1744 - accuracy: 0.9889 - val_loss: 0.0309 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, validation_data=val_ds) # entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01):\n",
    "\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(224,224,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors= keras.layers.GlobalAveragePooling2D()(base) \n",
    "    outputs = keras.layers.Dense(2)(vectors)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    optimazer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "313/313 [==============================] - 155s 491ms/step - loss: 0.2802 - accuracy: 0.9390 - val_loss: 0.1180 - val_accuracy: 0.9925\n",
      "\n",
      "\n",
      "0.001\n",
      "313/313 [==============================] - 170s 527ms/step - loss: 0.0632 - accuracy: 0.9858 - val_loss: 0.0201 - val_accuracy: 0.9962\n",
      "\n",
      "\n",
      "0.01\n",
      "313/313 [==============================] - 152s 478ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0085 - val_accuracy: 0.9975\n",
      "\n",
      "\n",
      "0.1\n",
      "313/313 [==============================] - 146s 459ms/step - loss: 0.1638 - accuracy: 0.9901 - val_loss: 0.3032 - val_accuracy: 0.9862\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for lr in [0.0001, 0.001,0.01,0.1]:\n",
    "    print(lr)\n",
    "\n",
    "    model = make_model(learning_rate=lr)\n",
    "    history = model.fit(train_ds, epochs=1, validation_data=val_ds)\n",
    "    scores[lr] = history.history\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best LR -> learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner= 100):\n",
    "\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(224,224,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors= keras.layers.GlobalAveragePooling2D()(base) \n",
    "\n",
    "    # 100 neuronas en la capa oculta \n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors) # capa oculta agregada lo que hace es que se agregan mas neuronas a la red para poder tener mas parametros y poder ajustar mejor los datos\n",
    "\n",
    "\n",
    "    outputs = keras.layers.Dense(2)(inner)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    optimazer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "313/313 [==============================] - 134s 421ms/step - loss: 0.0304 - accuracy: 0.9917 - val_loss: 0.0171 - val_accuracy: 0.9975\n",
      "\n",
      "\n",
      "100\n",
      "313/313 [==============================] - 140s 440ms/step - loss: 0.0622 - accuracy: 0.9914 - val_loss: 0.0916 - val_accuracy: 0.9787\n",
      "\n",
      "\n",
      "1000\n",
      "313/313 [==============================] - 145s 457ms/step - loss: 0.1428 - accuracy: 0.9904 - val_loss: 0.0121 - val_accuracy: 0.9987\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for size in [10, 100, 1000]:\n",
    "    print(size)\n",
    "    \n",
    "    model = make_model(learning_rate=learning_rate, size_inner=size)\n",
    "    history = model.fit(train_ds, epochs=1, validation_data=val_ds)\n",
    "    scores[size] = history.history\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best Dense layer -> DenseLayer = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization and Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner= 10, droprate=0.5):\n",
    "\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(224,224,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors= keras.layers.GlobalAveragePooling2D()(base) \n",
    "\n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors) # capa oculta agregada lo que hace es que se agregan mas neuronas a la red para poder tener mas parametros y poder ajustar mejor los datos\n",
    "    drop = keras.layers.Dropout(droprate)(inner) # dropout es una capa que se agrega para evitar el overfitting lo que hace es que apaga algunas neuronas de manera aleatoria\n",
    "\n",
    "    outputs = keras.layers.Dense(2)(drop)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    optimazer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "313/313 [==============================] - 132s 414ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.0114 - val_accuracy: 0.9987\n",
      "\n",
      "\n",
      "0.2\n",
      "313/313 [==============================] - 143s 448ms/step - loss: 0.1972 - accuracy: 0.9678 - val_loss: 0.0210 - val_accuracy: 0.9962\n",
      "\n",
      "\n",
      "0.5\n",
      "313/313 [==============================] - 147s 466ms/step - loss: 0.2887 - accuracy: 0.8563 - val_loss: 0.0264 - val_accuracy: 0.9975\n",
      "\n",
      "\n",
      "0.8\n",
      "313/313 [==============================] - 140s 439ms/step - loss: 0.5108 - accuracy: 0.7170 - val_loss: 0.0967 - val_accuracy: 0.9912\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "size = 10\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for droprate in [0.0, 0.2, 0.5, 0.8]:\n",
    "    print(droprate)\n",
    "    \n",
    "    model = make_model(learning_rate=learning_rate, size_inner=size, droprate=droprate)\n",
    "    history = model.fit(train_ds, epochs=1, validation_data=val_ds)\n",
    "    scores[size] = history.history\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best droprate -> droprate = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner= 10, input_size=224):\n",
    "\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    inputs = keras.Input(shape=(input_size,input_size,3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors= keras.layers.GlobalAveragePooling2D()(base) \n",
    "\n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors) # capa oculta agregada lo que hace es que se agregan mas neuronas a la red para poder tener mas parametros y poder ajustar mejor los datos\n",
    "\n",
    "    outputs = keras.layers.Dense(2)(inner)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "#############################################################\n",
    "\n",
    "    optimazer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer= optimazer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    '../data/data/Train',\n",
    "    target_size = (input_size,input_size),\n",
    "    batch_size =16\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    '../data/data/Validation',\n",
    "    target_size = (input_size,input_size),\n",
    "    batch_size = 16,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'mobileNet_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daniel\\Documents\\Universidad\\Cursos\\MLZoomCamp\\Capstone_Project_2\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 140s 219ms/step - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.0115 - val_accuracy: 0.9950\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 148s 238ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0076 - val_accuracy: 0.9987\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 150s 239ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0100 - val_accuracy: 0.9962\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 147s 235ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0581 - val_accuracy: 0.9900\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 143s 228ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.0462 - val_accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 142s 227ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.0116 - val_accuracy: 0.9987\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 146s 234ms/step - loss: 9.2797e-04 - accuracy: 0.9997 - val_loss: 0.0097 - val_accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 144s 231ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0033 - val_accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 145s 232ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0081 - val_accuracy: 0.9987\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 142s 228ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0065 - val_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "size = 10\n",
    "\n",
    "model = make_model(\n",
    "    input_size= 224,\n",
    "    learning_rate=learning_rate,\n",
    "    size_inner= size,\n",
    ")\n",
    "\n",
    "history = model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks= [checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
